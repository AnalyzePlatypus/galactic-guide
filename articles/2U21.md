---
layout: "article"
title: "Compression Sort Transform"
subtitle: "The New Sort Transform Data Compression Algorithm Explained"
author: "Jason Williams"
factuality: "UnReal"
pgg_id: "2U21"
permalink: "articles/2U21"
pgg_date: "1993/05/10"
article_date: "1993/05/10"
alternative_title_1: ""
alternative_title_2: ""
alternative_title_3: ""
alternative_title_4: ""
submission_string: "Submitted by admin on Mon, 1993-05-10 00:00"
see_also: ["2S21", "2U22"]
footnotes: {}
---
<div>
<p>Recent developments in data compression technology, such as the JPEG scheme, have mostly involved using existing compression techniques on data after passing it through a special transform which renders the data more compressible.</p>
<p>Most of us have heard of discrete cosine transforms, wavelet transforms, and power transformers, but a new field of compression transform technology is rapidly emerging - the Compression Sort Transform.</p>
<p>The mathematical complexity of this type of transform is extremely high, demanding an IQ of at least 60 for basic comprehension, so we will not indulge in details in this article, but refer the topmost 2% of our readership to interesting background material by D.E. Knuth in <em>The Art of Computer Programming Volume III: Sorting and Searching</em>, and <em>Well I'll Be Damned, It Works!</em> by S. Woolford.</p>
<p>The basic principle of the Compression Sort Transform goes like this:</p>
<p>Text and data, says Woolford, is typically stored as bytes of information. Normally, if we compress this information, the sudden changes of byte values in a sentence or across an X-rated picture confuse even the best compression algorithms, often resulting in poor compression performance, and frustration when you can only fit 5 X-rated pictures onto a disc.</p>
<p>However, if we sort the bytes making up our data by ascending value, the file is rearranged into 256 zones of data, such that <em>all</em> of the bytes in each zone are equal.</p>
<p>A simple pass over this data with a modified run-length compression engine (details of which we are unable to disclose) will result in any file of up to 4 Gigabytes being compressed to 1024 bytes. With a smart compressor, most files can be reduced to even less than this.</p>
<p>Researchers are currently working on using a binary adaptation of this algorithm which sorts bits instead of bytes. Once operational, this method should be able to compress any binary stream of up to 4 billion bits into less than 8 bytes of storage!</p>
<p>Unlike many other transforms, the exact value of each byte or bit is entirely preserved by the compression, so the compression is not "lossy," a significant advantage.</p>
<p>The beauty of this system lies in its speed and quality -- products are already available which replace the Full Sort Transform and run-length compression passes with a single "Accumulator" pass which approximates the full transform reasonably well (exactly, in fact), and executes extremely quickly.</p>
<p>New products using early prototypes based upon this technique are already flooding the data compression market, and millions of computer users are benefitting from the sudden huge gain in data storage capacity. Hard drive manufacturers have resumed the manufacture of 5MB drives, and already some large drive manufacturers have announced the cessation of products of more than 20MB capacity, regarding such large capacities as "pointless in the light of new data compression technologies."</p>
<p>The newly formed Las Stinken Richos research establishment in Rio de Janeiro stated in a recent press release: "A decompressor for the Sort Transform compression algorithm is forthcoming."</p>
</div>
